{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domien96/pyspark-exercise/blob/dev/pyspark_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark homework assignment\n",
        "\n",
        "## Context\n",
        "\n",
        "The goal of this assignment is to get view on your coding workflow & style.  Your main focus should be creating performant & robust code for data manipulations.  \n",
        "\n",
        "For a homework assignment, we cannot grant you access to our infrastructure (Cloudera data platform on prem: a spark cluster deployment on Yarn).  Since the focus is on development, we provided a template notebook to get up and running very quickly on Google Colab.  \n",
        "\n",
        "You have the freedom to perform this assignment on any spark3+ infrastructure.  If want to use a local or cloud setup, go for it!\n",
        "\n",
        "Some of the tasks are open for interpretation.  This allows us to assess business understanding and relevant field experience.  These tasks are not pass or fail checks.  During the interview we'll ask details about the choice(s) you made.\n",
        "\n",
        "For the assignment, you'll be working with store location data.  You might be familiar with the phrase \"Location, location, location\" from the real-estate context.  The same house can have a different selling price based on the location.  In fast moving consumer goods (FMCG), location is one of the most crucial aspects:\n",
        "\n",
        "* Proximity & accessibility to customers increases convenience\n",
        "* Proximity to competitors increases market pressure\n",
        "* It has impact on the supply chain\n",
        "\n",
        "## Evaluation criteria\n",
        "\n",
        "1. Software engineering\n",
        "   1. Clean code (e.g. using meaningful names)\n",
        "   1. Robust & efficient code\n",
        "   1. Styling (e.g. PEP8, or Google style guide)\n",
        "   1. Documentation(e.g. docstrings)\n",
        "   1. Design (e.g. SOLID principles)\n",
        "1. Workflow\n",
        "   1. How you use Git\n",
        "   1. How you structure your assignment\n",
        "   1. Owning mistakes\n",
        "   1. Rationale for design decisions\n",
        "   1. Making your solution accessible to others\n",
        "1. Business context\n",
        "   1. GDPR\n",
        "   1. Fast moving consumer goods\n",
        "1.(optional: own infra) System engineering\n",
        "   1. What setup did you use?\n",
        "   1. How did you set it up?\n",
        "\n",
        "## Deliverables we expect\n",
        "\n",
        "1. Private GitHub repo\n",
        "   1. Colab allows you to save to GitHub\n",
        "   1. Invite my username to your private repo as contributor\n",
        "1. README.md with relevant content\n",
        "1. Code relevant to the assignment\n"
      ],
      "metadata": {
        "id": "MMCkWQR0NQh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google colab spark setup"
      ],
      "metadata": {
        "id": "ZWyMNpBNON-p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3RiQgV1aNOJD"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import environ\n",
        "import findspark"
      ],
      "metadata": {
        "id": "V6xZShWarSWL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting environment variables\n",
        "environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "Cvm3vu7cNSzV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Init spark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "8y6JdEBrO43D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# spark.sql.repl.eagerEval.enabled: Property used to format output tables better\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"cg-pyspark-assignment\")\n",
        "    .master(\"local\")\n",
        "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
        "    .getOrCreate()\n",
        "  )\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "o5-SFL47PQfJ",
        "outputId": "5ea71bde-e01d-461e-a3a4-9078980b8847"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x78f828021060>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://91137f44344c:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>cg-pyspark-assignment</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the assignment data\n",
        "\n",
        "This will call the api and save the results in current working directory as .json files"
      ],
      "metadata": {
        "id": "QWtk5p_punuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/clp-places > clp-places.json\n",
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/okay-places > okay-places.json\n",
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/spar-places > spar-places.json\n",
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/dats-places > dats-places.json\n",
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/cogo-colpnts > cogo-places.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-UEk918o3Xt",
        "outputId": "a7859db0-e82f-4dd9-bf54-0d21ac92ff1b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  218k    0  218k    0     0   117k      0 --:--:--  0:00:01 --:--:--  117k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  143k    0  143k    0     0   107k      0 --:--:--  0:00:01 --:--:--  107k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  167k    0  167k    0     0   122k      0 --:--:--  0:00:01 --:--:--  122k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 89162    0 89162    0     0   105k      0 --:--:-- --:--:-- --:--:--  105k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  247k    0  247k    0     0   168k      0 --:--:--  0:00:01 --:--:--  168k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment instructions\n",
        "\n",
        "1. Download the data from api\n",
        "1. Create a logger object that logs to a file \"assignment.log\"\n",
        "   1. You can add whatever logging config you want or need\n",
        "   1. At least on Filehandler based on instructions\n",
        "1. implement get_data_by_brand function\n",
        "   1. Follow instructions in docstring\n",
        "   1. df_clp code line should work\n",
        "1. No more handholding ... :-)\n",
        "1. Create a single object (dataframe) that:\n",
        "   1. Contains data from **all brands**\n",
        "      1. Not every brand has the same columns!\n",
        "   1. Drop placeSearchOpeningHours\n",
        "   1. You can keep sellingPartners as an array\n",
        "   1. Extract \"postal_code\" from address\n",
        "   1. Create new column \"province\" derived from postal_code\n",
        "   1. Transform geoCoordinates into lat and lon column\n",
        "   1. One-hot-encode the handoverServices\n",
        "   1. Pretend houseNumber and streetName are GDPR sensitive.\n",
        "      1. How would you anonymize this data for unauthorized users?\n",
        "      1. (optional) Implement the above\n",
        "      1. How would you show the real data to authorized users?\n",
        "      1. (optional) Implement the above\n",
        "1. Save the end result as a parquet file\n",
        "   1. (optional)partitioning?\n",
        "\n",
        "**postal_code** logic:\n",
        "* \"Brussel\": 1000-1299  \n",
        "* \"Waals-Brabant\": 1300-1499  \n",
        "* \"Vlaams-Brabant\": 1500-1999, 3000-3499  \n",
        "* \"Antwerpen\": 2000-2999  \n",
        "* \"Limburg\": 3500-3999  \n",
        "* \"Luik\": 4000-4999  \n",
        "* \"Namen\": 5000-5999  \n",
        "* \"Henegouwen\": 6000-6599,7000-7999  \n",
        "* \"Luxemburg\": 6600-6999  \n",
        "* \"West-Vlaanderen\": 8000-8999  \n",
        "* \"Oost-Vlaanderen\": 9000-9999"
      ],
      "metadata": {
        "id": "VyvIVnyivCpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import statements should go here\n",
        "import logging, sys\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType, LongType, DoubleType, StructType"
      ],
      "metadata": {
        "id": "DlsAnfxdxsT3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(filename='log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n",
        "LOGGER = logging.getLogger()\n",
        "LOGGER.addHandler(logging.StreamHandler(sys.stdout))"
      ],
      "metadata": {
        "id": "ShwdnfjSxm2G"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "    StructField(\"address\", StructType([\n",
        "        StructField(\"streetName\", StringType(), True),\n",
        "        StructField(\"houseNumber\", StringType(), True),\n",
        "        StructField(\"postalcode\", StringType(), True),\n",
        "        StructField(\"cityName\", StringType(), True),\n",
        "        StructField(\"countryName\", StringType(), True),\n",
        "        StructField(\"countryCode\", StringType(), True),\n",
        "    ]), True),\n",
        "    StructField(\"branchId\", StringType(), True),\n",
        "    StructField(\"commercialName\", StringType(), True),\n",
        "    StructField(\"ensign\", StringType(), True),\n",
        "    StructField(\"geoCoordinates\", StructType([\n",
        "        StructField(\"latitude\", DoubleType(), True),\n",
        "        StructField(\"longitude\", DoubleType(), True)\n",
        "    ]), True),\n",
        "    StructField(\"handoverServices\", ArrayType(StringType()), True),\n",
        "    StructField(\"isActive\", StringType(), True),\n",
        "    StructField(\"moreInfoUrl\", StringType(), True),\n",
        "    StructField(\"placeId\", LongType(), True),\n",
        "    StructField(\"placeSearchOpeningHours\", ArrayType(StructType([\n",
        "        StructField(\"from\", StringType(), True),\n",
        "        StructField(\"till\", StringType(), True)\n",
        "    ])), True),\n",
        "    StructField(\"placeType\", StringType(), True),\n",
        "    StructField(\"routeUrl\", StringType(), True),\n",
        "    StructField(\"sellingPartners\", ArrayType(StringType()), True),\n",
        "    StructField(\"sourceStatus\", StringType(), True),\n",
        "    StructField(\"temporaryClosures\", ArrayType(StringType()), True),\n",
        "])\n"
      ],
      "metadata": {
        "id": "ydX6vwZKtM-n"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_by_brand(brand: str, logger: logging.Logger = LOGGER):\n",
        "  \"\"\"Fetch input data based on brand.\n",
        "\n",
        "  Please add a column to the data indicating the input brand\n",
        "  Please add minimum one sanity check for loading the data\n",
        "  Please log things you consider relevant\n",
        "\n",
        "  Args:\n",
        "      brand: allowed values are (clp, okay, spar, dats, cogo)\n",
        "      logger: Logger object for logging\n",
        "\n",
        "  Returns:\n",
        "      The relevant dataframe\n",
        "  \"\"\"\n",
        "  file_path = f'{brand}-places.json'\n",
        "  LOGGER.info(f\"Reading {file_path}\")\n",
        "  df = spark.read.schema(schema).json(file_path)\n",
        "  df = df.withColumn('brand', F.lit(brand))\n",
        "  return df"
      ],
      "metadata": {
        "id": "_wZhHFGFXkW0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brands = (\"clp\", \"okay\", \"spar\", \"dats\", \"cogo\")\n",
        "df_allBrands = get_data_by_brand(brand=brands[0], logger=LOGGER)\n",
        "for brand in brands[1:]:\n",
        "  LOGGER.info(f\"{brand} -> {df_allBrands.count()} rows\")\n",
        "  df_allBrands = df_allBrands.unionByName(get_data_by_brand(brand=brand, logger=LOGGER))\n",
        "LOGGER.info(f\"Total of {df_allBrands.count} rows\")"
      ],
      "metadata": {
        "id": "chz34qgi0vvK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_beforeProvince = df_allBrands.drop(\"placeSearchOpeningHours\").withColumn(\"postalCode\", df_allBrands.address.postalcode.cast(IntegerType()))"
      ],
      "metadata": {
        "id": "raCPEOOs8tWU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Leesbaarder in SQL, onderliggende performance identiek, UDF's lijken me teveel performance overhead\n",
        "df_beforeProvince.createOrReplaceTempView(\"vw_before_prov_col\")\n",
        "df_withProvince = spark.sql(\"\"\"\n",
        "SELECT  df.*,\n",
        "        CASE\n",
        "            WHEN postalCode<=999 THEN null\n",
        "            WHEN postalCode<=1299 THEN 'Brussel'\n",
        "            WHEN postalCode<=1499 THEN 'Waals-Brabant'\n",
        "            WHEN postalCode<=1999 THEN 'Vlaams-Brabant'\n",
        "            WHEN postalCode<=2999 THEN 'Antwerpen'\n",
        "            WHEN postalCode<=3499 THEN 'Vlaams-Brabant'\n",
        "            WHEN postalCode<=3999 THEN 'Limburg'\n",
        "            WHEN postalCode<=4999 THEN 'Luik'\n",
        "            WHEN postalCode<=5999 THEN 'Namen'\n",
        "            WHEN postalCode<=6599 THEN 'Henegouwen'\n",
        "            WHEN postalCode<=6999 THEN 'Luxemburg'\n",
        "            WHEN postalCode<=7999 THEN 'Henegouwen'\n",
        "            WHEN postalCode<=8999 THEN 'West-Vlaanderen'\n",
        "            WHEN postalCode<=9999 THEN 'Oost-Vlaanderen'\n",
        "            ELSE null\n",
        "        END AS province\n",
        "FROM vw_before_prov_col df\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "idUT5NgFMruj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_latlon = df_withProvince \\\n",
        "              .withColumn(\"lat\", df_withProvince.geoCoordinates.latitude) \\\n",
        "              .withColumn(\"lat\", df_withProvince.geoCoordinates.longitude) \\\n",
        "              .drop(\"geoCoordinates\")"
      ],
      "metadata": {
        "id": "zim4NNgrRGsN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_latlon.select(\"handoverServices\").distinct().collect()"
      ],
      "metadata": {
        "id": "mu4oDv9sZhyf",
        "outputId": "7f060d95-dbad-4aff-8ffd-3a292ddd6d2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(handoverServices=None),\n",
              " Row(handoverServices=['CSOP_ORDERABLE']),\n",
              " Row(handoverServices=['COLLECTNEXTDAY', 'PARCELCOLLECTNEXTDAY', 'HOMEDELNEXTDAYDRIVER']),\n",
              " Row(handoverServices=['COLLECTNEXTDAY']),\n",
              " Row(handoverServices=['HOMEDELNEXTDAYDRIVER', 'COLLECTNEXTDAY']),\n",
              " Row(handoverServices=['CSOP_ORDERABLE', 'COLLECTNEXTDAY']),\n",
              " Row(handoverServices=['PARCELCOLLECTNEXTDAY', 'COLLECTNEXTDAY', 'HOMEDELNEXTDAYDRIVER']),\n",
              " Row(handoverServices=['HOMEDELIVERYNEXTDAY', 'CSOP_ORDERABLE']),\n",
              " Row(handoverServices=['COLLECTNEXTDAY', 'HOMEDELNEXTDAYDRIVER']),\n",
              " Row(handoverServices=['COLLECTNEXTDAY', 'HOMEDELNEXTDAYDRIVER', 'PARCELCOLLECTNEXTDAY']),\n",
              " Row(handoverServices=['COLLECTNEXTDAY', 'CSOP_ORDERABLE']),\n",
              " Row(handoverServices=['COLLECTNEXTDAY', 'PARCELCOLLECTNEXTDAY']),\n",
              " Row(handoverServices=['CSOP_ORDERABLE', 'HOMEDELIVERYNEXTDAY']),\n",
              " Row(handoverServices=['HOMEDELIVERYNEXTDAY']),\n",
              " Row(handoverServices=['HOMEDELNEXTDAYDRIVER', 'COLLECTNEXTDAY', 'PARCELCOLLECTNEXTDAY']),\n",
              " Row(handoverServices=['HOMEDELNEXTDAYDRIVER', 'PARCELCOLLECTNEXTDAY', 'COLLECTNEXTDAY']),\n",
              " Row(handoverServices=['PARCELCOLLECTNEXTDAY', 'COLLECTNEXTDAY']),\n",
              " Row(handoverServices=['PARCELCOLLECTNEXTDAY', 'HOMEDELNEXTDAYDRIVER', 'COLLECTNEXTDAY'])]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "#encoder = OneHotEncoder(inputCols=[\"handoverServices\"],\n",
        "#                        outputCols=[\"handoverServices_ohe\"])\n",
        "#model = encoder.fit(df_latlon)\n",
        "#df_ohe = model.transform(df_latlon)\n",
        "df_ohe = df_latlon"
      ],
      "metadata": {
        "id": "Wv7pgeujR09v"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Anonymize\n",
        "df_latlon.write.format(\"parquet\").save(\"tbl_full_access\")\n",
        "df_latlon.withColumn(\"houseNumber\", F.lit(\"******\")).withColumn(\"streetName\", F.lit(\"******\")).write.format(\"parquet\").save(\"tbl_restricted_access\")"
      ],
      "metadata": {
        "id": "IXHtyfAtaQG1"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}